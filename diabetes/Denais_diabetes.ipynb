{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Project\n",
    "\n",
    "The purpose of this project is to demonstrate basic skills handling large datasets with pyspark. \n",
    "\n",
    "The first section using the 'rel.csv' dataset involves  data processing, dataset output into a new file, and simple analysis.\n",
    "\n",
    "The second section using the 'diabetes.csv' dataset inolves training and testing a random forest classifier for diabetes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "rel_data = spark.read.csv('./rel.csv', sep = ',', header = False, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|     _c0|     _c1|\n",
      "+--------+--------+\n",
      "|C0005790|C0005778|\n",
      "|C1255279|C3537249|\n",
      "|C1255446|C0002520|\n",
      "|C1255552|C0596019|\n",
      "|C1254417|C0004611|\n",
      "+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show data\n",
    "rel_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Order the pair of strings alphabetically. Given a pair (c1, c2),\n",
    "\n",
    "-if c1 <= c2, then the ordered pair is still (c1, c2)\n",
    "\n",
    "or\n",
    "\n",
    "-if c2 < c1, then the ordered pair is (c2, c1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from pyspark.sql.functions import col,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|      c0|      c1|\n",
      "+--------+--------+\n",
      "|C0005778|C0005790|\n",
      "|C1255279|C3537249|\n",
      "|C0002520|C1255446|\n",
      "|C0596019|C1255552|\n",
      "|C0004611|C1254417|\n",
      "+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_data_alpha = rel_data.select(\n",
    "    #sort for c0, placing lower value in c0 and renaming the column\n",
    "    when(col(\"_c0\") <= col(\"_c1\"), col(\"_c0\")).otherwise(col(\"_c1\")).alias(\"c0\"),\n",
    "    #sort for c1, placing higher value in c1 and renaming the column\n",
    "    when(col(\"_c0\") <= col(\"_c1\"), col(\"_c1\")).otherwise(col(\"_c0\")).alias(\"c1\")\n",
    ")\n",
    "\n",
    "rel_data_alpha.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of instances for each ordered pair and save the result\n",
    "in a plain text file (named as “pair-count.txt”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|      c0|      c1|count|\n",
      "+--------+--------+-----+\n",
      "|C0002210|C1255543|    2|\n",
      "|C0031507|C1254766|    2|\n",
      "|C0043375|C1255373|    2|\n",
      "|C0002520|C0523760|    2|\n",
      "|C0039350|C1278175|   10|\n",
      "+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Use groupby on the organized dataset to group by each unique pair and then count each group\n",
    "rel_data_counts = rel_data_alpha.groupby('c0','c1').count()\n",
    "\n",
    "rel_data_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save output as a txt file\n",
    "#convert dataframe into single string format rdd\n",
    "rel_data_rdd = rel_data_counts.rdd.map(lambda row: f'\"{row.c0}\",\"{row.c1}\" {row[\"count\"]}')\n",
    "\n",
    "#convert RDD to single partition and save as txt file\n",
    "rel_data_rdd.coalesce(1).saveAsTextFile('pair-count.txt')\n",
    "\n",
    "#output is \"part-00000\" in the pair-count.txt folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many unique ordered pairs are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12946540"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_data_counts.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Model\n",
    "\n",
    "In this section, we utilize a dataset containing clinical information to develop a random forest model for diabetes classification (presence vs. absence), evaluating the model with AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "dm_data = spark.read.csv('./diabetes.csv', sep=',', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preview data\n",
    "dm_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows where the column “BloodPressure”, “BMI” or “Glucose” is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create temp view of data for sql query\n",
    "dm_data.createOrReplaceTempView(\"dm_data\")\n",
    "\n",
    "#filter for data of interest (values >0) via sql query\n",
    "small_dm = spark.sql(\"SELECT * FROM dm_data WHERE `BloodPressure` > 0 AND `BMI` > 0 AND `Glucose` > 0\")\n",
    "\n",
    "small_dm.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|min(Glucose)|\n",
      "+------------+\n",
      "|          44|\n",
      "+------------+\n",
      "\n",
      "+------------------+\n",
      "|min(BloodPressure)|\n",
      "+------------------+\n",
      "|                24|\n",
      "+------------------+\n",
      "\n",
      "+--------+\n",
      "|min(BMI)|\n",
      "+--------+\n",
      "|    18.2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check that there are no zero values\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "small_dm.select(min('Glucose')).show()\n",
    "small_dm.select(min('BloodPressure')).show()\n",
    "small_dm.select(min('BMI')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the categorical column “Pregnancies” into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|Pregnancies_INDEX|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|              6.0|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|              0.0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|              8.0|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|              0.0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|              1.0|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "#create index column for pregnancies \n",
    "indexer = StringIndexer(inputCol = 'Pregnancies', outputCol = 'Pregnancies_INDEX')\n",
    "small_dm = indexer.fit(small_dm).transform(small_dm)\n",
    "\n",
    "small_dm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+---------------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|Pregnancies_INDEX|Pregnancies_VEC|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+---------------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|              6.0| (16,[6],[1.0])|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|              0.0| (16,[0],[1.0])|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|              8.0| (16,[8],[1.0])|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|              0.0| (16,[0],[1.0])|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|              1.0| (16,[1],[1.0])|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol='Pregnancies_INDEX', outputCol = 'Pregnancies_VEC')\n",
    "small_dm = encoder.fit(small_dm).transform(small_dm)\n",
    "\n",
    "small_dm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single column with all the features collated together using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome',\n",
       " 'Pregnancies_INDEX',\n",
       " 'Pregnancies_VEC']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+-------+\n",
      "|features                                                                       |Outcome|\n",
      "+-------------------------------------------------------------------------------+-------+\n",
      "|(25,[0,1,2,3,5,6,7,8,15],[6.0,148.0,72.0,35.0,33.6,0.627,50.0,6.0,1.0])        |1      |\n",
      "|(25,[0,1,2,3,5,6,7,9],[1.0,85.0,66.0,29.0,26.6,0.351,31.0,1.0])                |0      |\n",
      "|(25,[0,1,2,5,6,7,8,17],[8.0,183.0,64.0,23.3,0.672,32.0,8.0,1.0])               |1      |\n",
      "|(25,[0,1,2,3,4,5,6,7,9],[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0,1.0])         |0      |\n",
      "|(25,[1,2,3,4,5,6,7,8,10],[137.0,40.0,35.0,168.0,43.1,2.288,33.0,1.0,1.0])      |1      |\n",
      "|(25,[0,1,2,5,6,7,8,14],[5.0,116.0,74.0,25.6,0.201,30.0,5.0,1.0])               |0      |\n",
      "|(25,[0,1,2,3,4,5,6,7,8,12],[3.0,78.0,50.0,32.0,88.0,31.0,0.248,26.0,3.0,1.0])  |1      |\n",
      "|(25,[0,1,2,3,4,5,6,7,8,11],[2.0,197.0,70.0,45.0,543.0,30.5,0.158,53.0,2.0,1.0])|1      |\n",
      "|(25,[0,1,2,5,6,7,8,13],[4.0,110.0,92.0,37.6,0.191,30.0,4.0,1.0])               |0      |\n",
      "|(25,[0,1,2,5,6,7,8,19],[10.0,168.0,74.0,38.0,0.537,34.0,10.0,1.0])             |1      |\n",
      "|(25,[0,1,2,5,6,7,8,19],[10.0,139.0,80.0,27.1,1.441,57.0,10.0,1.0])             |0      |\n",
      "|(25,[0,1,2,3,4,5,6,7,9],[1.0,189.0,60.0,23.0,846.0,30.1,0.398,59.0,1.0])       |1      |\n",
      "|(25,[0,1,2,3,4,5,6,7,8,14],[5.0,166.0,72.0,19.0,175.0,25.8,0.587,51.0,5.0,1.0])|1      |\n",
      "|(25,[1,2,3,4,5,6,7,8,10],[118.0,84.0,47.0,230.0,45.8,0.551,31.0,1.0,1.0])      |1      |\n",
      "|(25,[0,1,2,5,6,7,8,16],[7.0,107.0,74.0,29.6,0.254,31.0,7.0,1.0])               |1      |\n",
      "|(25,[0,1,2,3,4,5,6,7,9],[1.0,103.0,30.0,38.0,83.0,43.3,0.183,33.0,1.0])        |0      |\n",
      "|(25,[0,1,2,3,4,5,6,7,9],[1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0,1.0])        |1      |\n",
      "|(25,[0,1,2,3,4,5,6,7,8,12],[3.0,126.0,88.0,41.0,235.0,39.3,0.704,27.0,3.0,1.0])|0      |\n",
      "|(25,[0,1,2,5,6,7,8,17],[8.0,99.0,84.0,35.4,0.388,50.0,8.0,1.0])                |0      |\n",
      "|(25,[0,1,2,5,6,7,8,16],[7.0,196.0,90.0,39.8,0.451,41.0,7.0,1.0])               |1      |\n",
      "+-------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns (excluding the outcome column)\n",
    "feature_cols = [col for col in small_dm.columns if col not in [\"Outcome\"]] \n",
    "\n",
    "# Apply VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "small_dm = assembler.transform(small_dm)\n",
    "\n",
    "# check data\n",
    "small_dm.select(\"features\", \"Outcome\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split the collated data into training (70%) and testing (30%) datasets, using 2017 as the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dm_train, small_dm_test = small_dm.randomSplit([0.7,0.3], seed = 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a random forest classifier with number of decision trees set to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----------+\n",
      "|            features|Outcome|prediction|\n",
      "+--------------------+-------+----------+\n",
      "|(25,[1,2,5,6,7,8,...|      0|       0.0|\n",
      "|(25,[1,2,5,6,7,8,...|      0|       0.0|\n",
      "|(25,[1,2,3,4,5,6,...|      0|       0.0|\n",
      "|(25,[1,2,3,4,5,6,...|      0|       0.0|\n",
      "|(25,[1,2,3,4,5,6,...|      0|       0.0|\n",
      "|(25,[1,2,3,4,5,6,...|      0|       0.0|\n",
      "|(25,[1,2,5,6,7,8,...|      0|       0.0|\n",
      "|(25,[1,2,5,6,7,8,...|      0|       0.0|\n",
      "|(25,[1,2,3,4,5,6,...|      0|       0.0|\n",
      "|(25,[1,2,3,5,6,7,...|      0|       0.0|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#create randomforest classifier\n",
    "\n",
    "RF = RandomForestClassifier(labelCol = 'Outcome',featuresCol='features', numTrees=20)\n",
    "\n",
    "#train model\n",
    "RF_model = RF.fit(small_dm_train)\n",
    "\n",
    "#test model\n",
    "test = RF_model.transform(small_dm_test)\n",
    "\n",
    "#show results:\n",
    "test.select('features', 'Outcome','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the random forest classifier using the ROC curve metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuROC 0.8507462686567163\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator  = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability',\n",
    "    labelCol='Outcome'\n",
    ")\n",
    "\n",
    "print('AuROC', evaluator.evaluate(test, {evaluator.metricName: 'areaUnderROC'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmi6318",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
